{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857b5006-6d04-4ae1-8c69-c37898f70ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/words.txt MapPartitionsRDD[1] at textFile at <console>:25\n",
       "res1: Long = 6\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile = sc.textFile(\"hdfs://localhost:9000/words.txt\")\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea64b871-006c-46e4-bea5-8d3553a6adac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(\"   spark kafka  \", \" kafka   spark pyspark \", \"                \", spark, \"\", \"APACHE Kafka APache SParK \")\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd1ccc6-cc82-4c56-b378-aac0003facbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lowerCaseRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at <console>:26\n",
       "res3: Array[String] = Array(spark kafka, kafka   spark pyspark, \"\", spark, \"\", apache kafka apache spark)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lowerCaseRdd = textFile.map(line => line.trim().toLowerCase())\n",
    "lowerCaseRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3ca536-48b6-47da-bfcf-df7d28bdd597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordArryRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at <console>:26\n",
       "res4: Array[Array[String]] = Array(Array(spark, kafka), Array(kafka, \"\", \"\", spark, pyspark), Array(\"\"), Array(spark), Array(\"\"), Array(apache, kafka, apache, spark))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordArryRdd = lowerCaseRdd.map(line => line.split(\" \"))\n",
    "wordArryRdd.collect()                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158f88b6-05c3-48d2-a0ca-141ff2e486a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:26\n",
       "res5: Array[String] = Array(spark, kafka, kafka, \"\", \"\", spark, pyspark, \"\", spark, \"\", apache, kafka, apache, spark)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordRdd = wordArryRdd.flatMap(arr => arr)\n",
    "wordRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96f345ed-688b-4907-abbd-f75ea3092907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[8] at map at <console>:30\n",
       "res8: Array[(String, Int)] = Array((spark,1), (kafka,1), (kafka,1), (spark,1), (pyspark,1), (spark,1), (apache,1), (kafka,1), (apache,1), (spark,1))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordPairRdd = wordRdd\n",
    "                    .filter(word => !word.isEmpty())\n",
    "                    .map(word => (word, 1))\n",
    "wordPairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9cef5f-acf5-4b01-8fff-5f945fa6d164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCountRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at reduceByKey at <console>:28\n",
       "res9: Array[(String, Int)] = Array((pyspark,1), (apache,2), (kafka,3), (spark,4))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCountRdd = wordPairRdd.reduceByKey((acc, value) => acc + value)\n",
    "wordCountRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2aa9ade-19da-4da6-9ff6-619466d45d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountRdd.saveAsTextFile(\"hdfs://localhost:9000/word-count-scala.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0a4a9-4b45-47ac-9dab-eb0563956162",
   "metadata": {},
   "outputs": [],
   "source": [
    "// hdfs dfs -ls /"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
