{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a277aec-eb77-4768-a7b9-11ca0bec8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2c8b36-b23d-4aa5-8783-3ab12816a9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 01:52:02 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.80.128 instead (on interface ens33)\n",
      "22/03/15 01:52:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/03/15 01:52:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/15 01:52:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "config.setMaster(\"local\").setAppName(\"Assigment2\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f847643d-25fa-4613-a8cf-3aba6ef8c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SYMBOL: string (nullable = true)\n",
      " |-- SERIES: string (nullable = true)\n",
      " |-- OPEN: string (nullable = true)\n",
      " |-- HIGH: string (nullable = true)\n",
      " |-- LOW: string (nullable = true)\n",
      " |-- CLOSE: string (nullable = true)\n",
      " |-- LAST: string (nullable = true)\n",
      " |-- PREVCLOSE: string (nullable = true)\n",
      " |-- TOTTRDQTY: string (nullable = true)\n",
      " |-- TOTTRDVAL: string (nullable = true)\n",
      " |-- TIMESTAMP: string (nullable = true)\n",
      " |-- TOTALTRADES: string (nullable = true)\n",
      " |-- ISIN: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stockDf = spark.read.format('csv')\\\n",
    "                    .option('header', True)\\\n",
    "                    .option('inferSchema', True)\\\n",
    "                    .load(\"hdfs://localhost:9000/assigment2\")\\\n",
    "                    .drop(\"_c13\")\n",
    "\n",
    "stockDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1445b2-7ffe-4d94-a47e-71a6823c5c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Convert Daily Stock data in CSV format to Parquet format. Partitions \"Year\" \"Month\" \"Day\"\n",
    "from pyspark.sql.functions import col, date_format\n",
    "\n",
    "stockDfParquet = stockDf.withColumn(\"year\", date_format(col(\"TIMESTAMP\"), \"yyyy\"))\\\n",
    "                        .withColumn(\"month\", date_format(col(\"TIMESTAMP\"), \"MM\"))\\\n",
    "                        .withColumn(\"day\", date_format(col(\"TIMESTAMP\"), \"dd\"))\\\n",
    "                        .write\\\n",
    "                        .partitionBy(\"Year\", \"Month\", \"Day\")\\\n",
    "                        .format(\"parquet\")\\\n",
    "                        .mode(\"overwrite\")\\\n",
    "                        .save(\"hdfs://localhost:9000/stock-Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcfe35b-2d57-4ac0-81a6-4b8acd4f3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>    (185 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------------+------------------+------------+------------------+\n",
      "|  TIMESTAMP|    SYMBOL|          PRICEGAIN|        PRICEGAINP|  VOLUMNGAIN|       VOLUMNGAINP|\n",
      "+-----------+----------+-------------------+------------------+------------+------------------+\n",
      "|07-MAR-2022|  ATNINTER|0.05000000000000002| 33.33333333333335|        null|              null|\n",
      "|25-FEB-2022|SREIBNPNCD|               40.0|              32.0|    33260.93|156.70638398115432|\n",
      "|25-FEB-2022|      RHFL|  67.69999999999999|28.771780705482357|         882| 0.576096690187349|\n",
      "|28-FEB-2022|    FELDVR| 2.3499999999999996| 22.59615384615384|        null|              null|\n",
      "|28-FEB-2022|  MODISNME| 13.399999999999999|22.558922558922557|        null|              null|\n",
      "|22-FEB-2022|  STEELCAS|  53.30000000000001| 22.39495798319328|    768336.1| 74.88968715701235|\n",
      "|25-FEB-2022|  FILDF2GP|0.21999999999999997| 22.22222222222222|        null|              null|\n",
      "|02-MAR-2022|KOTHARIPRO|  19.35000000000001| 21.96367763904655|109570997.65| 826.4501156024005|\n",
      "|24-FEB-2022|  ATALREAL|  24.05000000000001|21.666666666666675|        null|              null|\n",
      "|  23-Feb-22|  VAISHALI|  9.350000000000001|20.085929108485505|  24941312.8|29.027791059498014|\n",
      "|07-MAR-2022|IBULHSGFIN|              163.0|19.662243667068758|       29133| 80.48123541030706|\n",
      "|22-FEB-2022|     KIOCL|  40.35000000000002| 19.38040345821327|    52702142|29.686582975965504|\n",
      "|03-MAR-2022| OSIAHYPER|               40.5|19.194312796208532|     1246740| 216.1627019904293|\n",
      "|25-FEB-2022|       MCL|  4.400000000000002| 18.96551724137932|        null|              null|\n",
      "|03-MAR-2022|  STCINDIA|              17.25|             18.75|    10134836|16.260757497448402|\n",
      "|  23-Feb-22|GANGESSECU| 18.650000000000006|18.557213930348265| 10739616.35|25.790276893103126|\n",
      "|28-FEB-2022|   MAANALU|               21.5|18.399657680787335|        null|              null|\n",
      "|  23-Feb-22|      DSSL| 32.150000000000006|18.371428571428574| 25619120.75| 49.57493040385209|\n",
      "|22-FEB-2022|    AHLADA| 17.650000000000006|18.271221532091104|  3632984.85|  68.1421025163663|\n",
      "|03-MAR-2022| THEINVEST| 18.099999999999994|18.099999999999994|   2143274.2|14.734235698176153|\n",
      "+-----------+----------+-------------------+------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#2.\tFind the PriceGain, PriceGainP, VolumeGain, VolumeGainP of each stock for 10 days [Refer Point 1] from historical data, write the results to JSON, Parquet, ORC, \n",
    "#   CSV format in Hadoop. \n",
    "#   The column should be “Date”, “Symbol”, “PriceGain”, “PriceGainP”, “VolumnGain”, “VolumnGainP”, sorted by PriceGainP in descending order \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, desc, lead, row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"SYMBOL\").orderBy(\"TIMESTAMP\")\n",
    "\n",
    "stockDf = stockDf.withColumn(\"PRICEGAIN\", col(\"CLOSE\") - col(\"OPEN\"))\\\n",
    "                 .withColumn(\"PRICEGAINP\", (col(\"PRICEGAIN\") / col(\"OPEN\")) * 100)\\\n",
    "                 .withColumn(\"VOLUMNGAIN\", lead(\"TOTTRDVAL\", 2).over(windowSpec))\\\n",
    "                 .withColumn(\"VOLUMNGAINp\", col(\"VOLUMNGAIN\") / col(\"TOTTRDVAL\") * 100)\\\n",
    "                 .select(\"TIMESTAMP\", \"SYMBOL\", \"PRICEGAIN\", \"PRICEGAINP\", \"VOLUMNGAIN\", \"VOLUMNGAINP\")\\\n",
    "                 .sort(desc(\"PRICEGAINP\"))\n",
    "\n",
    "# to find the volumnGain we need to take the lead or lag\n",
    "stockDf.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2ee17b-05c9-4b62-89db-8f75349fb9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:====================================================> (129 + 1) / 133]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------------------+--------------------+\n",
      "|      Symbol|        DateFrom|               Gain|               GainP|\n",
      "+------------+----------------+-------------------+--------------------+\n",
      "|SILVERMIC_F1|2021-12-14 10:00|              -39.0|-0.00105241484682...|\n",
      "|SILVERMIC_F1|2021-09-28 10:00|               48.0|0.001316207282903948|\n",
      "|  SILVERM_F1|2021-03-01 09:00|              292.0|0.007111817746518255|\n",
      "|SILVERMIC_F1|2021-10-19 00:00|              361.0|0.009386770841621513|\n",
      "|  MCXMETLDEX|2021-12-23 09:00| 37.030000000377186|0.003628982546228337|\n",
      "|SILVERMIC_F2|2021-02-15 10:00|              208.0|0.005061003339045616|\n",
      "|  MCXCOMPDEX|2021-06-21 09:00|  6.739999999641441|0.001029807770587...|\n",
      "|  MCXCOMPDEX|2021-06-23 09:00|  32.16999999945983|0.004850218703019227|\n",
      "|  MCXCOMPDEX|2021-06-23 11:00| -10.67000000027474|-0.00158212243760...|\n",
      "|  MCXMETLDEX|2021-06-23 09:00|  92.94000000017695| 0.01062346175413482|\n",
      "| CRUDEOIL_F1|2021-03-08 00:00|                4.0|0.001354183241305...|\n",
      "| CRUDEOIL_F1|2021-03-09 11:00|              -30.0|-0.01045420014914659|\n",
      "|  SILVERM_F1|2022-01-28 11:00|              -41.0|-0.00109803262013...|\n",
      "|  SILVERM_F1|2021-10-26 10:00|              122.0|0.003074733928403...|\n",
      "|  MCXMETLDEX|2021-03-22 09:00|-21.799999999580905|-0.00265489070808...|\n",
      "|  MCXCOMPDEX|2021-09-23 09:00|-43.660000000032596|-0.00652218585905...|\n",
      "|  MCXMETLDEX|2021-09-28 10:00| -9.239999999874271|-9.51739151358123...|\n",
      "|SILVERMIC_F2|2021-08-27 10:00|               34.0|8.888693686204016E-4|\n",
      "|   SILVER_F1|2021-02-02 11:00|              435.0|0.010020739475286438|\n",
      "|  MCXBULLDEX|2021-07-09 10:00|-31.770000000484288|-0.00361107649128...|\n",
      "+------------+----------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. Use the Intraday 1 Min data from parquet format, calculate Gain, GainP for every hourly window for each stock. The possible column output should be, \n",
    "#    “SYMBOL”, “DATEFROM”, “DATETO”, \"Gain\", “GAINP” where are DATEFROM and DATETO are timestamp/date columns example, DATEFROM from Feb 01, 2022 10:00 AM to Feb 01, 2022 11:00 \n",
    "from pyspark.sql.functions import row_number, col, date_trunc, col, to_timestamp, concat, lit, date_format, sum, avg, max, min, mean, count\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, LongType\n",
    "\n",
    "intraDaySchema = StructType() \\\n",
    "        .add(\"Symbol\", StringType(), True)\\\n",
    "        .add(\"Date\", StringType(), True)\\\n",
    "        .add(\"Time\", StringType(), True)\\\n",
    "        .add(\"Open\", DoubleType(), True)\\\n",
    "        .add(\"High\", DoubleType(), True)\\\n",
    "        .add(\"Low\", DoubleType(), True)\\\n",
    "        .add(\"Close\", DoubleType(), True)\\\n",
    "        .add(\"Volume\", LongType(), True)\\\n",
    "        .add(\"OI\", LongType(), True)\n",
    "\n",
    "intraDayDf = spark.read.format('csv')\\\n",
    "                    .option(\"head\", True)\\\n",
    "                    .schema(intraDaySchema)\\\n",
    "                    .load(\"hdfs://localhost:9000/raw/*/*/*.txt\")\n",
    "\n",
    "intraDayDf = intraDayDf.withColumn(\"DateTimeStr\", concat( col(\"Date\"), lit(\" \"), col(\"Time\")))\\\n",
    "    .withColumn(\"DateTime\", to_timestamp(col(\"DateTimeStr\"), \"yyyMMdd hh:mm\" ))\\\n",
    "    .withColumn(\"DateFrom\", date_format(col(\"DateTime\").cast(\"timestamp\"), \"yyyy-MM-dd HH:00\"))\\\n",
    " \n",
    "\n",
    "intraDayDf.groupBy(\"Symbol\",\"DateFrom\") \\\n",
    "    .agg(sum(\"Open\").alias(\"SumOpen\"), \\\n",
    "        sum(\"Close\").alias(\"SumClose\"))\\\n",
    "    .withColumn(\"Gain\", col(\"SumClose\") - col(\"SumOpen\"))\\\n",
    "    .withColumn(\"GainP\", (col(\"Gain\") / col(\"SumOpen\")) * 100)\\\n",
    "    .drop(\"SumOpen\")\\\n",
    "    .drop(\"SumClose\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e1ab3-ecf6-496e-a4a4-2a09ede4ce0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
