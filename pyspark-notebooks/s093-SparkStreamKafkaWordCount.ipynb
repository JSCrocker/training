{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414f0f5e-718f-4287-8deb-c45421cffcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SCALA_VERSION = '2.12'\n",
    "SPARK_VERSION = '3.1.3'\n",
    "# Download Kafka Jar file, this for readStream.format(\"kafka\"), \"kafka\" is a driver\n",
    "# kafka driver code is part of Maven Jar file\n",
    "# https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10_2.12/3.1.3\n",
    "# pyspark-shell shall download the jar file behind..\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9f97b9-8217-478b-bd57-9144e7f49355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect kafka with spark with simple word count example\n",
    "# run on a terminal after starting kafka\n",
    "#     kafka-topics  --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test\n",
    "#     kafka-console-producer --bootstrap-server localhost:9092 --topic test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a9a3f5-91f5-4b29-9765-d453d95beb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a89041-b1f9-447d-857f-1b0164a95dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/12 03:17:21 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.80.128 instead (on interface ens33)\n",
      "22/03/12 03:17:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.1.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-220b936e-d61d-4ae2-b15d-b28cf38bf693;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 616ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-220b936e-d61d-4ae2-b15d-b28cf38bf693\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/12ms)\n",
      "22/03/12 03:17:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark groupBy has default setting for spark.sql.shuffle.partitions as 200\n",
    "# we set to  4, should NOT be done in production \n",
    "spark = SparkSession.builder.master(\"local[1]\")\\\n",
    "                            .config('spark.sql.shuffle.partitions', 4)\\\n",
    "                            .appName(\"SparkStreamingKafkaBasic\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abfcbac-1ed0-4837-94c7-4d6f523981d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from kafka, here spark is consumer for kafka topic called test\n",
    "# spark streaming works as dataframe/sql\n",
    "# group.id is consumer group id\n",
    "# subcribe is kafka topic\n",
    "# \"kafka\" driver is not available by default with spark, we need to download it, we did on cell 1\n",
    "kafkaDf = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "  .option(\"subscribe\", \"test\")\\\n",
    "  .option(\"group.id\", \"wordcount-group\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c62c8d-a1f3-4053-ba62-883997174d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# key and value are binary type, we need to CAST To STRING type\n",
    "# TimestampType values\n",
    "# CreateTime: Timestamp relates to message creation time as set by a Kafka client/producer\n",
    "# LogAppend\n",
    "kafkaDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "955c6853-5d7d-4c7a-a301-3dafe5a9816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kafka values, key are in binary format\n",
    "# we need to type case to STRING\n",
    "# we pick only value, ignore other column\n",
    "linesDf = kafkaDf.selectExpr(\"CAST(value AS STRING)\")\n",
    "linesDf.printSchema() # we get only value as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33cfb24e-c56a-4d09-8550-fb988dff100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# split line into word list\n",
    "# flatten word list into individual element as output, similar to flatMap\n",
    "wordsDf = linesDf.select(F.explode(F.split(linesDf.value,\" \")).alias(\"word\") )\n",
    "wordCountsDf = wordsDf.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3b2215-dd50-4b6f-bca1-be501b9790b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/12 03:19:13 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7be2c071-00c3-4f3b-b46a-6ec3a1ecd7cc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "echoOnconsole = wordCountsDf\\\n",
    "                .writeStream\\\n",
    "                .outputMode(\"complete\")\\\n",
    "                .format(\"console\")\\\n",
    "                .start() # start the query. spark will subscribe for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55bee9c-ec7a-42eb-8fd7-7ab89dfd38c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now publish the word count result (word, count columns) to kafka topic \"word-counts\", publish as json format\n",
    "# {\"word\": \"kafka\", \"count\": 8}\n",
    "\n",
    "# F is alias for all functions, we can access col by F.col \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# convert all the columns into json\n",
    "# * represent all columns ie word, count, \n",
    "# struct create a structure around word, count columns\n",
    "# to json convert structure to column\n",
    "# value is Kafka value part of message\n",
    "wordCountsToKafkaDf = wordCountsDf\\\n",
    "                    .selectExpr(\"to_json(struct(*)) as value\")\n",
    "\n",
    "wordCountsToKafkaDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810090f9-79f8-464f-ae14-4435e3fa1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/12 03:24:52 WARN StreamingQueryManager: Stopping existing streaming query [id=87df4503-54eb-4954-8aac-9cbf7b4ae8c0, runId=9606aa0f-e89e-40cc-a060-0d435eb09627], as a new run is being started.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f805c57c990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/12 03:25:06 WARN HDFSBackedStateStoreProvider: The state for version 14 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/03/12 03:25:06 WARN HDFSBackedStateStoreProvider: The state for version 14 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|  Friday|    1|\n",
      "|      eh|    1|\n",
      "|    will|    1|\n",
      "|   cause|    1|\n",
      "| working|    1|\n",
      "|     its|    2|\n",
      "|      to|    1|\n",
      "|    part|    1|\n",
      "|      be|    1|\n",
      "|starting|    1|\n",
      "|somewhat|    1|\n",
      "|  almost|    1|\n",
      "|      is|    1|\n",
      "|     and|    1|\n",
      "|    time|    1|\n",
      "|   about|    1|\n",
      "|    this|    1|\n",
      "|     hmm|    1|\n",
      "|       i|    1|\n",
      "|     you|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/12 03:25:06 WARN HDFSBackedStateStoreProvider: The state for version 14 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/03/12 03:25:06 WARN HDFSBackedStateStoreProvider: The state for version 14 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|  Friday|    1|\n",
      "|      eh|    1|\n",
      "|    will|    1|\n",
      "|   cause|    1|\n",
      "| working|    1|\n",
      "|     its|    2|\n",
      "|      to|    1|\n",
      "|    part|    1|\n",
      "|      be|    1|\n",
      "|starting|    1|\n",
      "|somewhat|    1|\n",
      "|  almost|    1|\n",
      "|      is|    1|\n",
      "|     and|    1|\n",
      "|    time|    1|\n",
      "|   about|    2|\n",
      "|     how|    1|\n",
      "|    this|    1|\n",
      "|     hmm|    1|\n",
      "|       i|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|  Friday|    1|\n",
      "|      eh|    1|\n",
      "|    will|    1|\n",
      "|   cause|    1|\n",
      "| working|    1|\n",
      "|     its|    2|\n",
      "|      to|    1|\n",
      "|    part|    1|\n",
      "|      be|    1|\n",
      "|starting|    1|\n",
      "|somewhat|    1|\n",
      "|      is|    1|\n",
      "|    time|    1|\n",
      "|   about|    2|\n",
      "|     you|    1|\n",
      "|  almost|    1|\n",
      "|     and|    1|\n",
      "|     how|    1|\n",
      "|    this|    1|\n",
      "|     hmm|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|  Friday|    1|\n",
      "|      eh|    1|\n",
      "|    will|    1|\n",
      "|   cause|    1|\n",
      "| working|    1|\n",
      "|     its|    2|\n",
      "|      to|    1|\n",
      "|    part|    1|\n",
      "|      be|    1|\n",
      "|starting|    1|\n",
      "|somewhat|    1|\n",
      "|      is|    1|\n",
      "|    time|    1|\n",
      "|   about|    2|\n",
      "|      in|    1|\n",
      "|     you|    1|\n",
      "|  almost|    1|\n",
      "|     and|    1|\n",
      "|     how|    1|\n",
      "|    this|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/13 00:26:21 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "22/03/13 00:26:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 192.168.80.128:34975 in 10000 milliseconds. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 192.168.80.128:34975 in 10000 milliseconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n"
     ]
    }
   ],
   "source": [
    "# checkpointLocation is for storing local state, for system restart, system failure in between\n",
    "# ensure to run kafka console consumer for topic word-count, commands are present in top of file\n",
    "wordCountsToKafkaDf.writeStream.format(\"kafka\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                     .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "                    .option(\"topic\", \"word-counts\")\\\n",
    "                    .option(\"checkpointLocation\", \"file:///tmp/spark\")\\\n",
    "                    .start()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3078c-4dd6-493a-ac71-c48da1e0174d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
