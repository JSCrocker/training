{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a74099-91aa-43ce-baab-a684a462f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b22f719-bd04-486f-b4f2-589ecc899232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark Database\\nSingle, local database. NOt shared across multiple notebooks, not shared across worker. \\nBuilt into spark working director, Generally used for simple development/learning purpose\\n\\nIn production, we will be using Hive Data Catalog\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Database\n",
    "Single, local database. NOt shared across multiple notebooks, not shared across worker. \n",
    "Built into spark working director, Generally used for simple development/learning purpose\n",
    "\n",
    "In production, we will be using Hive Data Catalog\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01934047-5dac-4df0-ac33-aeb0f4b23880",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark Local Database\n",
    "Only for dev only, not for production\n",
    "\n",
    "3 components involved\n",
    "\n",
    "1) metadata - database name, tables, columns, data types, location where the data is stored,\n",
    "    managed by HIVE, HIVE internaly uses derby db to store all metadata\n",
    "2) spark temporary location spark.local.dir, /home/ubuntu/spark-temp where temp data used internally is stored\n",
    "3) spark data warehouse where all the database data will be stored.\n",
    "    We can see database, tables, there data with metadata, \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604616c1-05b8-479e-b382-6d48986c8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"SparkDatabase\")\n",
    "\n",
    "# embedded, simple, LOCAL spark database/warehouse\n",
    "# spark will store tempoary files\n",
    "# enable hive support for sql database\n",
    "# enable hiveSupport Hive catalog to be embedded inside working directory\n",
    "config.set(\"spark.local.dir\", \"/home/ubuntu/spark-temp\")\n",
    "# spark data (not meta) does into /home/ubuntu/spark-warehouse\n",
    "config.set(\"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehouse\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "\n",
    "# enableHiveSupport() create a meta catalog/database using derby database\n",
    "# inside current working directory\n",
    "# multiple notebooks cant share at same time\n",
    "# inside pyspark-notebooks, you could see metastore_db\n",
    "# metastore will have meta data: database, tables, columns, data types\n",
    "# data located in hdfs or files system or s3\n",
    "# derby.Log - derby database log\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1cda555-2fb1-4869-b699-4bca4d164c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|stocklocaldb|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SHOW DATABASES\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a79944b-06c8-442b-b006-42260d93f5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/07 20:14:07 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/03/07 20:14:07 WARN ObjectStore: Failed to get database stocklocaldb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS stocklocaldb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0957b32a-cfe2-4653-91aa-9166d7e6bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/07 20:15:09 WARN HiveMetaStore: Location: file:/home/ubuntu/spark-warehouse/stocks specified for non-external table:stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Craete spark Managed table\n",
    "# insert, update ( delete will not work at 2.x)\n",
    "# need to type spark.sql to managed data\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS stocks(symbol STRING, industry STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e19de23-98aa-4bf5-a9d5-7d292449b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO stocks VALUES('INFY', 'IT')\n",
    "\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15fb840-a601-403d-bc3c-cc087db1fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|symbol|industry|\n",
      "+------+--------+\n",
      "|  INFY|      IT|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM  stocks\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614e2fc3-e78d-427a-b882-19b6b2007021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05423184-5b88-4a50-bc08-36c879a84a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebe228b2-2080-4bda-b7b7-ab2f061ae394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS stocklocaldb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1ea9a2a-1e65-4e44-9b3f-ed428cca1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f36cf09-f205-4824-a998-4c825bfc2b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4531e451-f270-431a-bd7e-cc6baa7e2960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/07 20:40:29 WARN ObjectStore: Failed to get database productdb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create DATABASE called prodcudb\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS productdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d125d831-9f0f-4b91-a0aa-0a2696c25bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a table called products with 3 fields (id int, name string, price int)\n",
    "# create table prodcutdb.products\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS productdb.products(id INT, product_name STRING, price INT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23159781-9ab9-432a-8ba5-5a80f342138a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert 2 recors into prodcutsdb.prodcuts\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO productdb.products VALUES(1, \"Water\", 2), \n",
    "                                (2, \"OJ\", 3)\n",
    "\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f66f299-418a-4987-bd80-676224b312a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| name|price|\n",
      "+---+-----+-----+\n",
      "|  1|Water|    2|\n",
      "|  2|   OJ|    3|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM productdb.products\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c49997fe-bb26-4d2f-8dcf-0c07d4dc5cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| name|price|\n",
      "+---+-----+-----+\n",
      "|  1|Water|    2|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM productdb.products WHERE price <= 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a507e74-6752-4cf8-8530-d1028eea3cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
