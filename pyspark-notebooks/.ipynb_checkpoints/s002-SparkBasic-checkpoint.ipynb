{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09cacd31-e07a-4545-986f-916b664157bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark\n",
    "# finds spark installation in the system\n",
    "# automatically set path, enviroment needed for pyspark\n",
    "# load spark libraries etc\n",
    "# /opt/spark-2.4.7....\n",
    "# good for single machine development, learning\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166ee5ff-9a1a-4771-b1ec-d4bf050078f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/28 21:19:08 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.80.128 instead (on interface ens33)\n",
      "22/02/28 21:19:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/02/28 21:19:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/28 21:19:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# create spark context - helps to create rdd, dag, job, task execute task etc\n",
    "# this code is called spark application or spark driver\n",
    "# every driver shall have ONLY ONE spark context\n",
    "from pyspark import SparkContext\n",
    "# local is execution mode, spark driver\n",
    "# spark executor runs in same JVM in smae machine/ not distributed\n",
    "# good for development or lreaning not for production\n",
    "sc = SparkContext(\"local\", \"SparkBasic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1713bf-2bad-4af6-ab6f-83fff8d1d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD, from hardcoded data\n",
    "# data is hardcoded in spark driver, this notebook\n",
    "# RDD shall be created in Executor process\n",
    "# Lazy evaluation\n",
    "#create RDD using parallelize methoid, by loading hardcoded data\n",
    "# RDD shall have partition(s)\n",
    "# the data hardcoded shall be loaded into partitions\n",
    "# at this moment, no data will be loaded, as this is lazy loading\n",
    "# when we apply action only then data will be loaded\n",
    "rdd = sc.parallelize([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e751e05b-9917-489a-9d1b-1d66b8d8f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter operation, we call this as TRANSFORMATION\n",
    "# TRANSFORMATION is code/task applied on partitioned data\n",
    "# filter will apply data (n) from partition to function supplied: lambda n : n % 2 == 1\n",
    "# lambda n: n % 2 == 1 will return either T or F\n",
    "# filter collect all the numbers where filter return true 1 3 5 7 9\n",
    "# LAZY evaluation, no partition, no data, not code loaded into exeuctor\n",
    "# until we apply ACTION on RDD\n",
    "oddRdd = rdd.filter(lambda n : n % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bed4ae-2179-4abe-bf4e-ccf10ef72dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# collect is an ACTION method\n",
    "# every ACTION methods create JOB\n",
    "# JOB is split into STAGES\n",
    "# each STAGE will have task\n",
    "# Taks will be running on executor on PARTITION\n",
    "# finally, collect bring the output back to DRIVER from EXECUTORs\n",
    "results = oddRdd.collect()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c35765f-a4b5-4191-b5fa-930061424f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# min is an ACTION\n",
    "# this create job, stages, DAG, tasks execute on cluster independently\n",
    "r = oddRdd.min()\n",
    "print(r) # print min of odd number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f91cc97-979c-4222-a0ee-8cbf714639c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "r2 = oddRdd.max()\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a74585b-9fa7-4926-af11-8573f2503dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "r3 = oddRdd.mean()\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6875c12b-cc85-48ca-80ef-ba3fe0080f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "r4 = oddRdd.sum()\n",
    "print(r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4e26e-a4b1-450f-a9c0-d7f7c02e5a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
