{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb5e025-002d-481b-a129-aa06209bb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e46cac-ab47-4621-abb7-88ff78d7dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.warehouse.dir\n",
      "22/03/08 01:30:21 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.80.128 instead (on interface ens33)\n",
      "22/03/08 01:30:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/03/08 01:30:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/08 01:30:22 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/03/08 01:30:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"MovieLensSQL\")\n",
    "\n",
    "config.set(\"spark.local.dir\", \"/home/ubuntu/spark-temp\")\n",
    "\n",
    "# while using \"hive.metastore.warehouse.dir, we should not use spark warehouse dir\n",
    "config.set(\"hive.metastore.uris\", \"thrift://localhost:9083\")\n",
    "config.set(\"hive.metastore.warehouse.dir\", \"hdfs://localhost:9000/user/hive/warehouse\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a942f583-bc6e-4346-b4f6-3c94db7cd8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /movies\n",
    "# hdfs dfs -ls /ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fda7eb-7343-452b-a233-7598d16fc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to create schema programatially instead of using inferSchema\n",
    "from pyspark.sql.types import StructType, LongType, StringType, IntegerType, DoubleType\n",
    "\n",
    "movieSchema = StructType()\\\n",
    "                    .add(\"movieId\", IntegerType(), True)\\\n",
    "                    .add(\"title\", StringType(), True)\\\n",
    "                    .add(\"genres\", StringType(), True)\n",
    "\n",
    "ratingSchema = StructType()\\\n",
    "                    .add(\"userId\", IntegerType(), True)\\\n",
    "                    .add(\"movieId\", IntegerType(), True)\\\n",
    "                    .add(\"rating\", DoubleType(), True)\\\n",
    "                    .add(\"timestamp\", LongType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc67a510-2015-44c9-a24b-8771bd21c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n",
      "+-------+----------------+--------------------+\n",
      "|movieId|           title|              genres|\n",
      "+-------+----------------+--------------------+\n",
      "|      1|Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|  Jumanji (1995)|Adventure|Childre...|\n",
      "+-------+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read movie data\n",
    "# read using dataframe with define schema\n",
    "# we can use folder path -  all csv in the folder read\n",
    "# use file path - only that file read\n",
    "\n",
    "#spark is a session, entry point for data frame/sql\n",
    "movieDf = spark.read.format('csv')\\\n",
    "                    .option('header', True)\\\n",
    "                    .schema(movieSchema)\\\n",
    "                    .load(\"hdfs://localhost:9000/movies\")\n",
    "\n",
    "movieDf.printSchema()\n",
    "movieDf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956223a9-5a64-431e-aabe-60ec337e5e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "+------+-------+------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingDf = spark.read.format('csv')\\\n",
    "                    .option('header', True)\\\n",
    "                    .schema(ratingSchema)\\\n",
    "                    .load(\"hdfs://localhost:9000/ratings\")\n",
    "\n",
    "ratingDf.printSchema()\n",
    "ratingDf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203ce5dd-e9ff-4a39-8789-f6c4ca787461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|     moviedb|\n",
      "|    moviesdb|\n",
      "|    ordersdb|\n",
      "|  productsdb|\n",
      "|      testdb|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we haven't configured hive yet\n",
    "# that is why we only have the defauly db\n",
    "spark.sql(\"\"\"\n",
    "    SHOW DATABASES\n",
    "\"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f9cea1-60bf-43d9-aef5-a4a3b0336c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temp table/temp view from data frame\n",
    "movieDf.createOrReplaceTempView(\"movies\")\n",
    "ratingDf.createOrReplaceTempView(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9d5589-1c54-41aa-88c9-62ffe64f3d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|   brands|      false|\n",
      "| default|employees|      false|\n",
      "| default|  moviedb|      false|\n",
      "| default| payroles|      false|\n",
      "|        |   movies|       true|\n",
      "|        |  ratings|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# two temp tables will be listed from default db\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb3e9b2-0e91-4135-b038-c51cbc88fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM movies LIMIT 5\n",
    "    \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9826fb49-a9c7-46d5-8e67-2e78f4edf7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM ratings LIMIT 5\n",
    "    \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8446d225-8925-41b4-ae10-c41ac908109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, rating=4.0),\n",
       " Row(userId=1, rating=4.0),\n",
       " Row(userId=1, rating=4.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark sql returns dataframe\n",
    "df1 = spark.sql(\"SElECT userId, rating FROM ratings\")\n",
    "df1.printSchema()\n",
    "df1.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6051c14-abb3-47a7-aee6-e122c2a831a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- avg_rating: double (nullable = true)\n",
      " |-- total_ratings: long (nullable = false)\n",
      "\n",
      "+-------+-----------------+-------------+\n",
      "|movieId|       avg_rating|total_ratings|\n",
      "+-------+-----------------+-------------+\n",
      "|   1580|3.487878787878788|          165|\n",
      "|   2366|             3.64|           25|\n",
      "|   3175|             3.58|           75|\n",
      "|   1088|3.369047619047619|           42|\n",
      "|  32460|             4.25|            4|\n",
      "+-------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# rating analytics for popular movies\n",
    "popularMovieDf = spark.sql(\"\"\"\n",
    "        SELECT movieId, avg(rating) AS avg_rating, count(userId) AS total_ratings\n",
    "        FROM ratings\n",
    "        GROUP BY movieId\n",
    "        \"\"\")     \n",
    "\n",
    "popularMovieDf.printSchema()\n",
    "popularMovieDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c70d59f-2547-4e75-b958-1f4b0845efa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rating analytics for popular movies\n",
    "# create a temporary view out of SQL SELECT statement\n",
    "# CTAS - CREATE TABLE AS\n",
    "# the popular_movies will have output analytical query\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW popular_movies AS\n",
    "    SELECT movieId, avg(rating) AS avg_rating, count(userId) AS total_ratings\n",
    "    FROM ratings\n",
    "    GROUP BY movieId\n",
    "    HAVING avg_rating >= 3.5 AND total_ratings >= 100\n",
    "    \"\"\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a11b8718-40b4-4984-aa7f-d32ae9ce030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "| default|        brands|      false|\n",
      "| default|     employees|      false|\n",
      "| default|       moviedb|      false|\n",
      "| default|      payroles|      false|\n",
      "|        |        movies|       true|\n",
      "|        |popular_movies|       true|\n",
      "|        |       ratings|       true|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c768a3ec-722a-4f5d-8711-afd95fc8f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------+\n",
      "|movieId|        avg_rating|total_ratings|\n",
      "+-------+------------------+-------------+\n",
      "|    858|         4.2890625|          192|\n",
      "|   1270| 4.038011695906433|          171|\n",
      "|   1265| 3.944055944055944|          143|\n",
      "|    588|3.7923497267759565|          183|\n",
      "|    296| 4.197068403908795|          307|\n",
      "+-------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM popular_movies\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa1b7b5d-a67c-45d3-ba95-d6393fce82bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============================================>       (171 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+-------------+\n",
      "|movieId|               title|        avg_rating|total_ratings|\n",
      "+-------+--------------------+------------------+-------------+\n",
      "|    318|Shawshank Redempt...| 4.429022082018927|          317|\n",
      "|    858|Godfather, The (1...|         4.2890625|          192|\n",
      "|   2959|   Fight Club (1999)| 4.272935779816514|          218|\n",
      "|   1221|Godfather: Part I...|  4.25968992248062|          129|\n",
      "|  48516|Departed, The (2006)| 4.252336448598131|          107|\n",
      "|   1213|   Goodfellas (1990)|              4.25|          126|\n",
      "|    912|   Casablanca (1942)|              4.24|          100|\n",
      "|  58559|Dark Knight, The ...| 4.238255033557047|          149|\n",
      "|     50|Usual Suspects, T...| 4.237745098039215|          204|\n",
      "|   1197|Princess Bride, T...| 4.232394366197183|          142|\n",
      "|    260|Star Wars: Episod...| 4.231075697211155|          251|\n",
      "|    527|Schindler's List ...|             4.225|          220|\n",
      "|   1208|Apocalypse Now (1...| 4.219626168224299|          107|\n",
      "|   2329|American History ...| 4.217054263565892|          129|\n",
      "|   1196|Star Wars: Episod...|4.2156398104265405|          211|\n",
      "|   1198|Raiders of the Lo...|            4.2075|          200|\n",
      "|   1193|One Flew Over the...| 4.203007518796992|          133|\n",
      "|   1089|Reservoir Dogs (1...| 4.202290076335878|          131|\n",
      "|    296| Pulp Fiction (1994)| 4.197068403908795|          307|\n",
      "|   2571|  Matrix, The (1999)| 4.192446043165468|          278|\n",
      "+-------+--------------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# join to get the movie title\n",
    "spark.sql(\"\"\"\n",
    "    SELECT movies.movieId, title, avg_rating, total_ratings\n",
    "    FROM popular_movies\n",
    "    INNER JOIN movies ON popular_movies.movieID = movies.movieId\n",
    "    ORDER BY avg_rating DESC\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d092abf4-b925-4e2f-8cc0-f6bdfac1a860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE A PERMANENT MANAGED TABLE in HIVE CATALOG\n",
    "# data will be stored in /user/hive/warehouse/moviedb/popular_movies\n",
    "# CTAS create table as select\n",
    "# FIXME\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW most_popular_movies AS \n",
    "    SELECT movies.movieId, title, avg_rating, total_ratings\n",
    "    FROM popular_movies\n",
    "    INNER JOIN movies ON popular_movies.movieID = movies.movieId\n",
    "    ORDER BY avg_rating DESC\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a6a3cee-5075-445b-8148-3942c229d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==================================>                   (128 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+-------------+\n",
      "|movieId|               title|       avg_rating|total_ratings|\n",
      "+-------+--------------------+-----------------+-------------+\n",
      "|    318|Shawshank Redempt...|4.429022082018927|          317|\n",
      "|    858|Godfather, The (1...|        4.2890625|          192|\n",
      "|   2959|   Fight Club (1999)|4.272935779816514|          218|\n",
      "|   1221|Godfather: Part I...| 4.25968992248062|          129|\n",
      "|  48516|Departed, The (2006)|4.252336448598131|          107|\n",
      "+-------+--------------------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM most_popular_movies\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5647ec-4349-4454-815c-29aabdb4092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:==>                                                     (4 + 1) / 110]22/03/08 01:45:29 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:===>                                                    (6 + 1) / 110]22/03/08 01:45:29 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:====>                                                   (9 + 1) / 110]22/03/08 01:45:29 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)\n",
      "22/03/08 01:45:29 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)\n",
      "22/03/08 01:45:29 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)\n",
      "[Stage 29:======>                                                (13 + 1) / 110]22/03/08 01:45:29 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)\n",
      "[Stage 29:========>                                              (16 + 1) / 110]22/03/08 01:45:29 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:===============>                                       (31 + 1) / 110]22/03/08 01:45:30 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:=============================>                         (58 + 1) / 110]22/03/08 01:45:32 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:==============================>                        (61 + 1) / 110]22/03/08 01:45:32 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:==================================>                    (68 + 1) / 110]22/03/08 01:45:33 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)\n",
      "22/03/08 01:45:33 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:====================================>                  (73 + 1) / 110]22/03/08 01:45:33 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:=======================================>               (79 + 1) / 110]22/03/08 01:45:33 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:577)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:573)\n",
      "22/03/08 01:45:33 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:==========================================>            (84 + 1) / 110]22/03/08 01:45:34 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "22/03/08 01:45:34 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "22/03/08 01:45:34 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "22/03/08 01:45:34 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 29:==================================================>   (103 + 1) / 110]22/03/08 01:45:35 WARN DFSClient: Caught exception \n",
      "java.lang.InterruptedException\n",
      "\tat java.lang.Object.wait(Native Method)\n",
      "\tat java.lang.Thread.join(Thread.java:1252)\n",
      "\tat java.lang.Thread.join(Thread.java:1326)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:609)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:546)\n",
      "[Stage 32:==========================================>           (157 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+-------------+\n",
      "|movieId|               title|       avg_rating|total_ratings|\n",
      "+-------+--------------------+-----------------+-------------+\n",
      "|    318|Shawshank Redempt...|4.429022082018927|          317|\n",
      "|    858|Godfather, The (1...|        4.2890625|          192|\n",
      "|   2959|   Fight Club (1999)|4.272935779816514|          218|\n",
      "|   1221|Godfather: Part I...| 4.25968992248062|          129|\n",
      "|  48516|Departed, The (2006)|4.252336448598131|          107|\n",
      "+-------+--------------------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get dataframe from table/temp view\n",
    "most_popular_moviesDf = spark.table(\"most_popular_movies\")\n",
    "most_popular_moviesDf.write\\\n",
    "                        .mode('overwrite')\\\n",
    "                        .saveAsTable(\"moviedb.most_popular_movies\")\n",
    "most_popular_moviesDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d60255c9-8658-4eac-81ad-c59255783bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+-------------+\n",
      "|movieId|               title|        avg_rating|total_ratings|\n",
      "+-------+--------------------+------------------+-------------+\n",
      "|   4896|Harry Potter and ...|3.7616822429906542|          107|\n",
      "|   1198|Raiders of the Lo...|            4.2075|          200|\n",
      "|    293|LÃ©on: The Profess...| 4.018796992481203|          133|\n",
      "|   6539|Pirates of the Ca...| 3.778523489932886|          149|\n",
      "|   4993|Lord of the Rings...| 4.106060606060606|          198|\n",
      "+-------+--------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM moviedb.most_popular_movies\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f0be01-83b3-494e-baf4-3499a2630c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "| moviedb|most_popular_movies|      false|\n",
      "| moviedb|             movies|      false|\n",
      "| moviedb|            ratings|      false|\n",
      "| moviedb|            reviews|      false|\n",
      "|        |most_popular_movies|       true|\n",
      "|        |             movies|       true|\n",
      "|        |     popular_movies|       true|\n",
      "|        |            ratings|       true|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN moviedb\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1bc65-14ad-4081-80fb-895baa295522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On hive CLI\n",
    "\"\"\"\n",
    "select * form moviedb.most_popular_movies;\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
